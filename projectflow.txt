-------------------------Setting up project structure and notebook analysis---------------------------

1.  Creating GitHub repo, cloning it locally  
2.  Creating virtual environment named 'calories-ml' conda create -n calories-ml python=3.10 -y
3.  Activating the virtual environment - conda activate calories-ml 
4.  Installing packages inside venv - pip install numpy pandas matplotlib seaborn ipykernel scikit-learn xgboost notebook
5.  Conecting new venv with Jupyter - python -m ipykernel install --user --name=calories-ml --display-name "Python (calories-ml)"
6.  Starting Jupyter notebook while inside venv - jupyter notebook
7.  Analysis in notebook inlcuding EDA, preprocessing, model training and evaluation
8.  First gid add . - commit - push 

-------------------------Setup MLFlow on Dagshub ( Experiments Part)---------------------------

9.  DAagshub link: https://dagshub.com/dashboard 
10. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
11. pip install dagshub & mlflow in virtual environment
12. First experiment - file called experiment1, connecting base model (xgboost) and doing few runs with changing hyperparameters (example with n_estimators change)
13. Second experiment - connecting and comparing base model vs another models and comparing results in MLFlow (we get best results for xgboost algorithm)
14. Third experiment - xgboost hyperparameters tuning to get the best model for predictions
15. Finishing and collecting all info in our first notebook where whole EDA started 
16. on terminal - dvc init 
17. create a local folder as "local_s3" (temporary work) 
18. on terminal - "dvc remote add -d mylocal local_s3"

------------------------Building an aplication---------------------------------------------------

19. Creating src directory where I will store all components (data_ingestion file, data_preprocessing etc..)
20. Creating src/data/data_ingestion.py (testing by loading local files then loading from AWS s3 bucket). Then create IAM user and create access and secret keys. 
    Inside src/connections create config.json file and add your AWS credentials for IAM user. Add data to s3 bucket before testing.
21. Creating src/data/data_preprocessing.py 
22. Creating src/features/feature_engineering.py 
23. Creating src/model/model_building.py 
24. Creating src/model/model_evaluation.py (testing locally first ,production code commented out, and connecting to dagshub mlflow)
25. Creating src/model/model_register.py (also testing locally first, production code commented out, and connecting to dagshub mlflow )
26. Creating dvc.yaml file (all components combined in pipeline)
27. Run dvc.yaml file using command dvc repro
28. pip install - dvc[s3] & awscli 
29. Checking/deleting dvc remote - [dvc remote list & dvc remote remove mylocal] , because we are going to use aws s3 bucket
30. Set aws cred using command - aws configure (user your access key and secret key that you created for IAM user, also can be found in config.json file) 
31. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>
32. Run dvc status and dvc commit commands (check dvc lock file if everything is tracked)
33. dvc push (tracking files pushed to s3 bucket) - image name ./3_dvc_push_to_s3_bucket
34. Create new directory flask_app and inside that all files related to application. Change in flask_app/app.py file line 45, stages = ["Staging"]. 
35. Inside flask_app folder create requirements.txt file that will be used by docker (separated from project root folder requirements.txt file to reduce the size of Docker image).
    Do commands pip install pipreqs and after that cd flask_app & do "pipreqs . --force", this will put only neccessery packages for the app into requirements.txt file
36. pip install -r flask_app/requirements.txt (so you can install packages needed for aplication local testing) 
37. run app from root folder with command python -m flask_app.app (local app testing) - image 1_app_local_testing 
38. pip freeze > requirements.txt (saved in root folder, requirements for the whole project, you can install at the beggining of the project) 
39. Create .github/workflows/ci.yaml file (till line 38, dvc repro part)
40. Create key token on Dagshub for auth: Go to dagshub repo > Your settings > Tokens > Generate new token (name Calories-Burnt-Pred)
    >> Make sure to save token 
    >> Add this auth token to github project repository, go to security/secret&var/Actions and finally update on ci file (name )
41. git add ./commit/push but before pushing code check for down bellow: - image 4_CICD_first_pipeline 
    1. gitignore must have /data/ to not truck files in that folder (this is root data folder not scr/data so do not write /data in gitignore write /data/)
    2. create dagshub token and add to github repository secrets 
    3. check your script and comment local code but uncomment production code that uses dugshub token (model_evaluation.py, model_register.py,app.py)
    4. add AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, S3_BUCKET_NAME and AWS_REGION from your config.json file into GitHub secrets
    5. After succesfull CICD pipeline you should get new model on MLFLOW in stageing phase

42. Add directories "tests"&"scripts" and files within them. This will contain our test related scripts for CI.yaml. Each time when we run ci.yaml we need to test model and promote best one from Staging to Production phase. We are also testing the aplication.
43. Add, commit and push your code so new CICD pipeline is triggered and ci.yaml fun run till line 57. Images 5_CICD_second_pipeline and 6_Model_pushed_to_production_stage 
44. Change in flask_app/app.py file line 45, stages = ["Production"] . Before this step it was  stages = ["Staging"]

--------------------------- Moving to Docker -----------------------------------------------
45. Create dockerfile and start docker-desktop in background (first we will test image locally to check for potential bugs). Production code in Dockerfile is commented out in this phase.
46. Go to root dir and: "docker build -t calories-burnt-pred-app:latest ."    
47. Run image and create container with command: "docker run -p 8888:5000 calories-burnt-pred-app:latest" and your app will run on link http://127.0.0.1:8888/ check images 8_Docker_Container_Built and 9_Aplication_run_inside_Docker
48. Create AWS ECR repository called calories-burnt-pred-proj and add into GitHub secrets new one under name ECR_REPOSITORY value is calories-burnt-pred-proj. Another secrets you need to add is your AWS_ACCOUNT_ID
    Also add this permission to the IAM user: AmazonEC2ContainerRegistryFullAccess
49. Execute CICD pipeline till the stage where we build and push image to ECR (ci.yaml created till line 85) - images 10_Aplication_dockerized_and_pushed_on_ECR and 11_Docker_image_appear_on_ECR 

--------------------------- Setup required before moving to EKS deployment-------------------

50. * Run the following in PowerShell to see the AWS CLI path being used: Get-Command aws
    If the path points to your Anaconda environment (e.g., C:\Users\Personal\anaconda3\...), itâ€™s a conflicting installation.
    * Uninstall Python-Based AWS CLI(Remove the conflicting AWS CLI from your Python environment): pip uninstall awscli
    * Verify it's uninstalled: aws --version
    * Update Environment Variables:
    > Make sure the .msi AWS CLI path is in your system PATH (usually installed at C:\Program Files\Amazon\AWSCLIV2\aws.exe).
    > Add it to your PATH if missing: Open Control Panel > System > Advanced System Settings > Environment Variables. Under "System Variables," find Path, and add the AWS CLI path: C:\Program Files\Amazon\AWSCLIV2\
    > Test AWS CLI Again: aws --version

51. Download kubectl: Invoke-WebRequest -Uri "https://dl.k8s.io/release/v1.28.2/bin/windows/amd64/kubectl.exe" -OutFile "kubectl.exe"
    * Locate the Download: Get-Location
    * Move kubectl.exe to a directory in your system PATH, such as C:\Windows\System32, so command is: Move-Item -Path .\kubectl.exe -Destination "C:\Windows\System32"
    * Test if kubectl is properly installed: kubectl version --client


52. Download eksctl: Invoke-WebRequest -Uri "https://github.com/weaveworks/eksctl/releases/download/v0.158.0/eksctl_Windows_amd64.zip" -OutFile "eksctl.zip"
    * Extract eksctl: Expand-Archive -Path .\eksctl.zip -DestinationPath .
    * Move the extracted eksctl.exe file to C:\Windows\System32 or any folder in your system PATH: Move-Item -Path .\eksctl.exe -Destination "C:\Windows\System32\eksctl.exe"

53. Verify installations
    * Verify AWS CLI: aws --version
    * Verify kubectl: kubectl version --client
    * Verify eksctl: eksctl version

------------------------------------Create an EKS cluster--------------------------

54. Use down bellow command for EKS cluster creation 
    * eksctl create cluster --name flask-app-cluster --region us-east-1 --nodegroup-name flask-app-nodes --node-type t3.small --nodes 1 --nodes-min 1 --nodes-max 1 --managed

55. Update kubectl Config(Once the cluster is created, eksctl will automatically update your kubectl config file. However, you can verify and set it manually using:)
aws eks --region us-east-1 update-kubeconfig --name flask-app-cluster (This ensures your kubectl is pointing to the correct cluster.)

56. Check EKS Cluster Configuration Ensure you can access your EKS cluster by running down bellow command
    aws eks list-clusters

57. Verify the cluster status (you can also check on AWS EKS UI but here is how to check in cmd):
    aws eks --region us-east-1 describe-cluster --name flask-app-cluster --query "cluster.status"

58. Check cluster connectivity (you will see only one node becuase we request one node in our eks cluster creation command):
    kubectl get nodes

59. Check the namespaces:
    kubectl get namespaces

60. Deploy the app on EKS via CICD pipeline 
  >> edit ci.yaml, full file creation, creation of deployment.yaml, edit dockerfile (comment out code for local use and uncomment for production use)
  >> Also edit the security group for nodes and edit inbound rule for 5000 port (go into EC2, find instance that was automatically created during EKS cluster creation, go inside instance and in Security tab enter security groups. Edit inbound rules) - image 13_Change_inbound_rules_on_EKS_cluster_EC2
  >> For every case go into ec2 instance and add AmazonEC2ContainerRegistryFullAccess policy to the instance that represent our EKS cluster node so it can pull images from ECR
  >> In deployment.yaml file update line 19 with your docker image url on ECR
  >> Now by doing gid . , commit and push you will deploy the app - image 14_Application_deployment_on_AWS_EKS_server

61. Verify the deployment:
    kubectl get pods
    kubectl get svc

62. Once the LoadBalancer service is up, get the external IP:
    kubectl get svc flask-app-service
    > I got aa41626d38f0245b48ed006810eac4bc-1107424181.us-east-1.elb.amazonaws.com so I will add port 5000 at the end 
    > final ip for the app is http://aa41626d38f0245b48ed006810eac4bc-1107424181.us-east-1.elb.amazonaws.com:5000



------------------------------ Prometheus Server Setup ----------------------------

63. Launch an Ubuntu EC2 Instance for Prometheus: t3.medium,  20GB of disk space (general-purpose SSD), Security Group: Allow inbound access on ports: 9090 for Prometheus Web UI, 22 for SSH access

64. SSH into the EC2 Instance(optional or connect directly to ec2 server alternatively):
    ssh -i your-key.pem ubuntu@your-ec2-public-ip

65. Update packages: sudo apt update && sudo apt upgrade -y (command in EC2 instance terminal)

66. Download Prometheus on our new EC2 instance:
    > wget https://github.com/prometheus/prometheus/releases/download/v2.46.0/prometheus-2.46.0.linux-amd64.tar.gz
    > tar -xvzf prometheus-2.46.0.linux-amd64.tar.gz
    > mv prometheus-2.46.0.linux-amd64 prometheus

67. Move files to standard paths:
    sudo mv prometheus /etc/prometheus
    sudo mv /etc/prometheus/prometheus /usr/local/bin/

68. Create Prometheus Configuration (we need to edit Prometheus yaml file): 
    >> Open the file for editing: sudo nano /etc/prometheus/prometheus.yml
    >> Edit the File:

    global:
      scrape_interval: 15s

    scrape_configs:
      - job_name: "flask-app"
        static_configs:
          - targets: ["aa41626d38f0245b48ed006810eac4bc-1107424181.us-east-1.elb.amazonaws.com:5000"]  # Replace with your app's External IP

    >> Save the File: ctrl+o -> enter -> ctrl+x
    >> Verify the Changes: cat /etc/prometheus/prometheus.yml

69. Locate the Prometheus Binary(Run the following command to find where the prometheus executable is installed):
    > which prometheus
    This should return the full path to the prometheus binary, such as /usr/local/bin/Prometheus

70. Run Prometheus with the config file:
    > /usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml

71. Now go to instance that you create for Prometheus and inside security groups add inbound rule for port 9090 and add access from anywhere
    Now you copy publi ipv4 adress of prometheus instance and add port 9090 and you will access Prometheus that is scraping your EKS deployed application (image 18_Prometheus_started_scraping_application_metrics)


--------------------------------- Grafana Server Setup ----------------------------

72. Launch an Ubuntu EC2 Instance for Grafana: t3.medium,  20GB of disk space (general-purpose SSD), Security Group: Allow inbound access on ports: 3000 for Grafana Web UI, 22 for SSH access

73. SSH into the EC2 Instance(optional or connect directly through AWS UI to ec2 server alternatively):
    ssh -i your-key.pem ubuntu@your-ec2-public-ip

74. Update and upgrade system packages:
    sudo apt update && sudo apt upgrade -y

75. Download Grafana: wget https://dl.grafana.com/oss/release/grafana_10.1.5_amd64.deb
    (this is a stable version for now; adjust the link if necessary.)

76. Install Grafana: sudo apt install ./grafana_10.1.5_amd64.deb -y

77. Start the Grafana service: sudo systemctl start grafana-server

78. Enable Grafana to start on boot: sudo systemctl enable grafana-server

79. Verify the service is running: sudo systemctl status grafana-server

80. Open Grafana web UI: http://<ec2-public-ip>:3000 (username/pass - admin) so link is 52.3.230.129:3000 for my Graphana

81. Add Prometheus as a Data Source: http://44.199.231.198:9090
    click - Save and Test | Get started with building dashboards.

------------------------------AWS Resource Cleanup---------------------------

* Delete deployment - kubectl delete deployment flask-app
* Delete service - kubectl delete service flask-app-service
* Delete env var - kubectl delete secret calories-secret
* Delete EKS Cluster - eksctl delete cluster --name flask-app-cluster --region us-east-1
* Verify Cluster Deletion - eksctl get cluster --region us-east-1
* Delete artifacts of ECR and S3 (optional - delete ECR and S3)
* Validate if Cloud Formation stacks are deleted.
* Confirm service termination on AWS support chat.
